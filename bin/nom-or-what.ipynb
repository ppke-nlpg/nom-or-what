{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ligeti/bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# setting up the environment\n",
    "import os\n",
    "import re\n",
    "from os.path import join, isfile, splitext\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from nomorwhat import *\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "macro_config_file = 'macros.yml'\n",
    "input_file = 'test_input.txt'\n",
    "output_file = 'test_output.txt'\n",
    "orig_annot_file = 'test_full_annot.txt'\n",
    "merged_output_file = 'test_merged_output.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ligeti/bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# reading macros\n",
    "with open(macro_config_file, 'r') as fin:\n",
    "    macros = yaml.load(fin)\n",
    "my_sentences = readtext(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_macro(macro_name, anal):\n",
    "    macro_res = False\n",
    "    # print(macro_name, anal)\n",
    "    if macros['macros'][macro_name]['type'] == 'list':\n",
    "        if anal[0] in macros['macros'][macro_name]['value']:\n",
    "            macro_res = True\n",
    "        else:\n",
    "            macro_res = False\n",
    "    elif macros['macros'][macro_name]['type'] == 'regex':\n",
    "        if macros['macros'][macro_name]['regexp_type'] == 'search':\n",
    "            macro_res = re.search(macros['macros'][macro_name]['value'], anal[2])\n",
    "        else:\n",
    "            pass\n",
    "    elif macros['macros'][macro_name]['type'] == 'ends':\n",
    "        macro_res = anal[2].endswith(macros['macros'][macro_name]['value'])\n",
    "    elif macros['macros'][macro_name]['type'] == 'complex':\n",
    "        if macros['macros'][macro_name]['compl_type'] == 'and': \n",
    "            all_true = True\n",
    "            for macro in macros['macros'][macro_name]['sub_macros']:\n",
    "                if not check_macro(macro,anal):\n",
    "                    all_true = False\n",
    "                    break\n",
    "            macro_res = all_true\n",
    "        elif macros['macros'][macro_name]['compl_type'] == 'or':\n",
    "            one_true = False\n",
    "            for macro in macros['macros'][macro_name]['sub_macros']:\n",
    "                if check_macro(macro,anal):\n",
    "                    one_true = True\n",
    "                    break\n",
    "            macro_res = one_true\n",
    "    elif macros['macros'][macro_name]['type'] == 'neg':\n",
    "        macro_res = not check_macro(macros['macros'][macro_name]['sub_macro'],anal)\n",
    "            \n",
    "    return macro_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "na\n"
     ]
    }
   ],
   "source": [
    "#macro_name = 'nu_mn'\n",
    "#macros['macros'][macro_name]\n",
    "\n",
    "if check_macro('not_gen', ['az', 'az', 'DET']):\n",
    "    print(\"ugyesvagy\")\n",
    "if check_macro('NPMod', ['szép', 'szép', 'MN.NOM']):\n",
    "    print(\"ugyesvagy\")\n",
    "if check_macro('NOM', [\"almák\", 'alma', 'FN.PL.NOM']):\n",
    "    print(\"ugyesvagy\")\n",
    "if check_macro('not_kop_v', ['teszek', 'tesz', 'IGE.SG1']):\n",
    "    print(\"ugyesvagy\")\n",
    "if check_macro('def_art', ['a', 'a', 'DET']):\n",
    "    print('ugyesvagy')\n",
    "if not check_macro('def_art', ['az', 'az', 'FN_NM.NOM']):\n",
    "    print('ugyesvagy')\n",
    "    \n",
    "if not check_macro('Adj_tree', ['szépek', 'szép', 'MN.PL.NOM']):\n",
    "    print('ugyesvagy')\n",
    "\n",
    "if check_macro('NPMod', ['1848_49-es', '1848_49-es', 'MN.NOM']):\n",
    "    print(\"na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NUM_rules(window, curr_POS):\n",
    "    first_right_word, first_right_lemma, first_right_annot = window[0]\n",
    "    second_right_word, second_right_lemma, second_right_annot = window[1]\n",
    "    \n",
    "    if re.search(\"FN|SZN|MN|NU\", first_right_annot): # if the next token is a nominal, or a postposition, the word gets a \"none\"\n",
    "        curr_POS = curr_POS.replace('NOM', 'semmi')\n",
    "        \n",
    "    elif check_macro('not_kop_v', window[0]): # if the next token is verb, but not a copula, the word is a nominative\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "        \n",
    "    elif check_macro('def_art', window[0]) or re.search('HA', first_right_annot): # if the next token is a definite article or an adverb, the word is a nominative\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "        \n",
    "    else:\n",
    "        curr_POS = curr_POS.replace('NOM', 'defsemmi')\n",
    "        \n",
    "    return curr_POS\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADJ_rules(window, curr_POS):\n",
    "    first_right_word, first_right_lemma, first_right_annot = window[0]\n",
    "    second_right_word, second_right_lemma, second_right_annot = window[1]\n",
    "    \n",
    "    if check_macro('NUs', window[0]): # if the next token is postposition(al like element), the word gets a 'none'\n",
    "        curr_POS = curr_POS.replace('NOM', 'semmi')\n",
    "    \n",
    "    elif check_macro('not_kop_v', window[0]): # if the next token is a verb, but not a copula, the word is a nominative\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    elif check_macro('szn', window[0]): # before a numeral: default value\n",
    "        curr_POS = curr_POS.replace('NOM', 'nulla')\n",
    "        \n",
    "        if check_macro('PSE', window[1]): # if the second token has a poss. suff.\n",
    "            curr_POS = curr_POS.replace('nulla', 'gen')\n",
    "        else: # otherwise\n",
    "            curr_POS = curr_POS.replace('nulla', 'nom')\n",
    "    \n",
    "    elif check_macro('full_stop', window[0]): # if the next token is a fix outsider, this word must be the end of an NP, thus 'nom'\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    else: # otherwise\n",
    "        curr_POS = curr_POS.replace('NOM', 'defsemmi')\n",
    "        if check_macro('PUNCT', window[1]): # if there is a punct.mark in the window\n",
    "            curr_POS = curr_POS.replace('defsemmi', 'semmi')\n",
    "            \n",
    "    return curr_POS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOUN_rules(window, curr_POS, curr_word):\n",
    "    first_right_word, first_right_lemma, first_right_annot = window[0]\n",
    "    second_right_word, second_right_lemma, second_right_annot = window[1]\n",
    "    \n",
    "    if check_macro('NUs', window[0]): # if the next token is postposition(al like element), the word gets a 'none'\n",
    "        curr_POS = curr_POS.replace('NOM', 'semmi')\n",
    "        \n",
    "    elif check_macro('not_gen', curr_word):\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    elif check_macro('PSE', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'gen')\n",
    "        \n",
    "    elif check_macro('not_kop_v', window[0]) or check_macro('pl', window[0]) or first_right_annot == 'IK':\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    elif check_macro('TULN', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'nulla')\n",
    "        \n",
    "        if not check_macro('cimunevu', window[1]):\n",
    "            curr_POS = curr_POS.replace('nulla', 'nom')\n",
    "    \n",
    "    elif check_macro('NPMod', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'nulla')\n",
    "    \n",
    "        if check_macro('PSE', window[1]) and not re.search('PSe3', curr_POS):\n",
    "            curr_POS = curr_POS.replace('nulla', 'gen')\n",
    "        elif check_macro('V', window[1]) or check_macro('PUNCT', window[1]) or check_macro('vonatk', window[1]) or re.search('PSe3', curr_POS):\n",
    "            curr_POS = curr.POS.replace('nulla', 'nom')       \n",
    "        \n",
    "    elif check_macro('full_stop', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    else:\n",
    "        curr_POS = curr_POS.replace('NOM', 'nulla')\n",
    "        \n",
    "        if check_macro('V', window[1]) or check_macro('PUNCT', window[1]) or check_macro('vonatk', window[1]) or re.search('PSe3', curr_POS):\n",
    "            curr_POS = curr.POS.replace('nulla', 'nom')       \n",
    "        \n",
    "        \n",
    "    return curr_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nom_or_what(s):\n",
    "    sent = format_sents(s) \n",
    "    new_sent = [] # to store a sentence with the novel tags\n",
    "    to_write_later = [] # to store the NOM token and the window to create the annotation file later\n",
    "    for i in range(len(sent)):\n",
    "        \n",
    "        token = sent[i]\n",
    "        curr_word = token[0]\n",
    "        curr_lemma = token[1]\n",
    "        curr_POS = token[2]\n",
    "        \n",
    "        if curr_word != \"#\": # if the given token is not the end of a sentence\n",
    "        \n",
    "            window = sent[i + 1:i + 3]\n",
    "\n",
    "            \n",
    "            if check_macro('NOM', token): # if the given token is a suffixless nominal\n",
    "                              \n",
    "                if check_macro('Noun_tree', token): # if the given token is a noun, or a plural adjective, participle\n",
    "                    curr_POS = NOUN_rules(window, curr_POS, curr_word)\n",
    "        \n",
    "                elif check_macro('Adj_tree', token): # if the given token is a singular adjective or participle\n",
    "                    curr_POS = ADJ_rules(window, curr_POS)\n",
    "        \n",
    "                elif check_macro('Num_tree', token): # if the given token is a numeral\n",
    "                    curr_POS = NUM_rules(window, curr_POS)\n",
    "                \n",
    "                new_token = curr_word + ' ' + curr_lemma + ' ' + curr_POS\n",
    "                to_write_later.append( (window, new_token ) ) # a tuple of the window of the given token and the full (novel) annotation of the token\n",
    "                \n",
    "        new_sent.append(curr_word + '/' + curr_lemma + '/' + curr_POS)\n",
    "    \n",
    "    return (new_sent, to_write_later)\n",
    "\n",
    "\n",
    "    # print(' '.join(new_sent))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_annot_file(new_sent, window, outp, i):\n",
    "    outp.writelines(str(i) + '. ' + ' '.join([token.split('/')[0] for token in new_sent])+ '\\n')\n",
    "    [outp.writelines(('-' + nom.split()[0] + ' ' + window[0][0] + ' ' + window[1][0] + '\\n', nom + '\\n', nom + '\\n', nom + '\\n')) for (window, nom) in to_write]\n",
    "    outp.writelines(('\\n', '\\n'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 sent, demo\n",
    "new_sent, to_write = nom_or_what('Több/sok/SZN._FOK.NOM levelet/levél/FN.ACC írt/ír/IGE.Me3 a/a/DET másik/másik/MN_NM.NOM honti/honti/MN.NOM jóbarátnak/jóbarát/FN.DAT és/és/KOT munkatársnak/munkatárs/FN.DAT ,/,/WPUNCT Pajor_István/Pajor_István/TULN.NOM 1848_49-es/1848_49-es/MN.NOM nemzetőrnek/nemzetőr/FN.DAT ,/,/WPUNCT megyei/megyei/MN.NOM tisztviselőnek/tisztviselő/FN.DAT ,/,/WPUNCT írónak/író/FN.DAT és/és/KOT költőnek/költő/FN.DAT is/is/HA ././SPUNCT')\n",
    "with open (output_file, 'w') as outp:\n",
    "    write_to_annot_file(new_sent,to_write, outp, 1)\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 sents, demo, file\n",
    "with open(input_file, 'r') as inp, open(output_file, 'w') as outp:\n",
    "    for i, line in enumerate(inp, start = 1):\n",
    "        new_sent, to_write = nom_or_what(line)\n",
    "        write_to_annot_file(new_sent, to_write, outp, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_annot_with_outp():\n",
    "    annot = open(orig_annot_file, 'r').readlines()\n",
    "    outp = open(output_file, 'r').readlines()\n",
    "    merged_outp = open(merged_output_file, 'w')\n",
    "    print(len(annot))\n",
    "    j = -1\n",
    "    for i in range(len(annot)):\n",
    "        if i == j:\n",
    "            merged_outp.write(outp[i])\n",
    "        else:\n",
    "            merged_outp.write(annot[i])\n",
    "        if annot[i].startswith('-'):\n",
    "            j = i+3\n",
    "    merged_outp.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge_annot_with_outp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "merge_annot_with_outp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
