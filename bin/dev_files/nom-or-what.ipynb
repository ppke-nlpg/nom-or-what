{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the environment\n",
    "import os\n",
    "import re\n",
    "# from os.path import join, isfile, splitext\n",
    "# from os import listdir\n",
    "# from IPython.core.display import display, HTML\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from nomorwhat import *\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "macro_config_file = 'macros.yml'\n",
    "# input_file = 'test_input.txt'\n",
    "input_file = 'test_100_input.txt'\n",
    "input_file = 'test_500_input.txt'\n",
    "input_file = 'test_1000_input.txt'\n",
    "# output_file = 'test_output.txt'\n",
    "output_file = 'test_100_output.txt'\n",
    "output_file = 'test_500_output.txt'\n",
    "output_file = 'test_1000_output.txt'\n",
    "# orig_annot_file = 'test_full_annot.txt'\n",
    "orig_annot_file = 'test_100_full_annot.txt'\n",
    "orig_annot_file = 'test_500_full_annot.txt'\n",
    "orig_annot_file = 'test_1000_full_annot.txt'\n",
    "# merged_output_file = 'test_merged_output.txt'\n",
    "merged_output_file = 'test_100_merged_output.txt'\n",
    "merged_output_file = 'test_500_merged_output.txt'\n",
    "merged_output_file = 'test_1000_merged_output.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ligeti/bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# reading macros\n",
    "with open(macro_config_file, 'r') as fin:\n",
    "    macros = yaml.load(fin)\n",
    "# my_sentences = readtext(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_macro(macro_name, anal):\n",
    "    macro_res = False\n",
    "    # print(macro_name, anal)\n",
    "    if macros['macros'][macro_name]['type'] == 'list':\n",
    "        if anal[0] in macros['macros'][macro_name]['value']:\n",
    "            macro_res = True\n",
    "        else:\n",
    "            macro_res = False\n",
    "    elif macros['macros'][macro_name]['type'] == 'regex':\n",
    "        if macros['macros'][macro_name]['regexp_type'] == 'search':\n",
    "            macro_res = re.search(macros['macros'][macro_name]['value'], anal[2])\n",
    "        else:\n",
    "            pass\n",
    "    elif macros['macros'][macro_name]['type'] == 'ends':\n",
    "        macro_res = anal[2].endswith(macros['macros'][macro_name]['value'])\n",
    "    elif macros['macros'][macro_name]['type'] == 'complex':\n",
    "        if macros['macros'][macro_name]['compl_type'] == 'and': \n",
    "            all_true = True\n",
    "            for macro in macros['macros'][macro_name]['sub_macros']:\n",
    "                if not check_macro(macro,anal):\n",
    "                    all_true = False\n",
    "                    break\n",
    "            macro_res = all_true\n",
    "        elif macros['macros'][macro_name]['compl_type'] == 'or':\n",
    "            one_true = False\n",
    "            for macro in macros['macros'][macro_name]['sub_macros']:\n",
    "                if check_macro(macro,anal):\n",
    "                    one_true = True\n",
    "                    break\n",
    "            macro_res = one_true\n",
    "    elif macros['macros'][macro_name]['type'] == 'neg':\n",
    "        macro_res = not check_macro(macros['macros'][macro_name]['sub_macro'],anal)\n",
    "            \n",
    "    return macro_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ugyesvagyez\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyesvagyte\n",
      "ugyesvagy\n",
      "ugyesvagy\n",
      "ugyI\n",
      "ugyesvagy\n",
      "na\n"
     ]
    }
   ],
   "source": [
    "#macro_name = 'nu_mn'\n",
    "#macros['macros'][macro_name]\n",
    "\n",
    "if check_macro('not_gen', ['ez', 'ez', 'FN_NM.NOM']):\n",
    "    print(\"ugyesvagyez\")\n",
    "if check_macro('NPMod', ['szép', 'szép', 'MN.NOM']):\n",
    "    print(\"ugyesvagy\")\n",
    "if check_macro('NOM', [\"almák\", 'alma', 'FN.PL.NOM']):\n",
    "    print(\"ugyesvagy\")\n",
    "if check_macro('not_kop_v', ['válaszolt', 'válaszol', 'IGE.Me3']):\n",
    "    print(\"ugyesvagyte\")\n",
    "if check_macro('def_art', ['a', 'a', 'DET']):\n",
    "    print('ugyesvagy')\n",
    "if not check_macro('def_art', ['az', 'az', 'FN_NM.NOM']):\n",
    "    print('ugyesvagy')\n",
    "if check_macro('full_stop', ['.', '.', 'SPUNCT']):\n",
    "    print('ugyI')\n",
    "if not check_macro('Adj_tree', ['szépek', 'szép', 'MN.PL.NOM']):\n",
    "    print('ugyesvagy')\n",
    "\n",
    "if check_macro('NPMod', ['1848_49-es', '1848_49-es', 'MN.NOM']):\n",
    "    print(\"na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NUM_rules(window, curr_POS):\n",
    "    first_right_word, first_right_lemma, first_right_annot = window[0]\n",
    "    second_right_word, second_right_lemma, second_right_annot = window[1]\n",
    "    \n",
    "    if re.search(\"FN|SZN|MN|NU\", first_right_annot): # if the next token is a nominal, or a postposition, the word gets a \"none\"\n",
    "        curr_POS = curr_POS.replace('NOM', 'none')\n",
    "        \n",
    "    elif check_macro('not_kop_v', window[0]): # if the next token is verb, but not a copula, the word is a nominative\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "        \n",
    "    elif check_macro('def_art', window[0]): # or re.search('HA', first_right_annot): # if the next token is a definite article or an adverb, the word is a nominative\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "\n",
    "        \n",
    "    else:\n",
    "        curr_POS = curr_POS.replace('NOM', 'defnone')\n",
    "        if check_macro('PUNCT', window[1]) or check_macro('def_art', window[1]) or re.search('HA', second_right_annot) or check_macro('V', window[1]):\n",
    "            curr_POS = curr_POS.replace('defnone', 'none')\n",
    "        \n",
    "    return curr_POS\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADJ_rules(window, curr_POS):\n",
    "    first_right_word, first_right_lemma, first_right_annot = window[0]\n",
    "    # print(window)\n",
    "    second_right_word, second_right_lemma, second_right_annot = window[1]\n",
    "    \n",
    "    if check_macro('NUs', window[0]): # if the next token is postposition(al like element), the word gets a 'none'\n",
    "        curr_POS = curr_POS.replace('NOM', 'none')\n",
    "    \n",
    "    elif check_macro('not_kop_v', window[0]): # if the next token is a verb, but not a copula, the word is a nominative\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    elif check_macro('szn', window[0]): # before a numeral: default value\n",
    "        curr_POS = curr_POS.replace('NOM', 'suff')\n",
    "        \n",
    "        if check_macro('PSE', window[1]): # if the second token has a poss. suff.\n",
    "            curr_POS = curr_POS.replace('suff', 'gen')\n",
    "        else: # otherwise\n",
    "            curr_POS = curr_POS.replace('suff', 'nom')\n",
    "    \n",
    "    elif check_macro('full_stop', window[0]): # if the next token is a fix outsider, this word must be the end of an NP, thus 'nom'\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "    \n",
    "    else: # otherwise\n",
    "        curr_POS = curr_POS.replace('NOM', 'defnone')\n",
    "        if check_macro('full_stop', window[1]) or check_macro('V', window[1]):# or check_macro('esetragos', window[1]): # if there is a punct.mark in the window\n",
    "            curr_POS = curr_POS.replace('defnone', 'none')\n",
    "            \n",
    "    return curr_POS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOUN_rules(window, curr_POS, curr_word, token):\n",
    "    first_right_word, first_right_lemma, first_right_annot = window[0]\n",
    "    second_right_word, second_right_lemma, second_right_annot = window[1]\n",
    "    if check_macro('NUs', window[0]): # if the next token is postposition(al like element), the word gets a 'none'\n",
    "        curr_POS = curr_POS.replace('NOM', 'none')\n",
    "        \n",
    "    elif check_macro('not_gen', token):\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "        # print('notgen')\n",
    "    \n",
    "    elif check_macro('PSE', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'gen')\n",
    "        \n",
    "    elif check_macro('not_kop_v', window[0]) or check_macro('pl', window[0]):# or first_right_annot == 'IK':\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "        # print('notkop')\n",
    "    elif check_macro('TULN', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'suff')\n",
    "        \n",
    "        if not check_macro('cimunevu', window[1]):\n",
    "            curr_POS = curr_POS.replace('suff', 'nom')\n",
    "            # print('cimunevu')\n",
    "    elif check_macro('NPMod', window[0]):\n",
    "        curr_POS = curr_POS.replace('NOM', 'suff')\n",
    "        # print(\"hurra\")\n",
    "        if check_macro('PSE', window[1]) and not re.search('PSe3', curr_POS):\n",
    "            curr_POS = curr_POS.replace('suff', 'gen')\n",
    "        elif check_macro('V', window[1]) or check_macro('PUNCT', window[1]) or check_macro('vonatk', window[1]) or re.search('PSe3', curr_POS):\n",
    "            curr_POS = curr_POS.replace('suff', 'nom')       \n",
    "            # print('V')\n",
    "    elif check_macro('full_stop', window[0]) or first_right_word == ':' or first_right_word == '!':\n",
    "        curr_POS = curr_POS.replace('NOM', 'nom')\n",
    "        # print('fullstop')\n",
    "    else:\n",
    "        curr_POS = curr_POS.replace('NOM', 'suff')\n",
    "        \n",
    "        if check_macro('V', window[1]) or check_macro('PUNCT', window[1]) or check_macro('vonatk', window[1]) or re.search('PSe3', curr_POS) or second_right_word == \"#\":\n",
    "            curr_POS = curr_POS.replace('suff', 'nom')       \n",
    "            # print('masoikV')\n",
    "        \n",
    "    return curr_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nom_or_what(s):\n",
    "    sent = format_sents(s) \n",
    "    new_sent = [] # to store a sentence with the novel tags\n",
    "    to_write_later = [] # to store the NOM token and the window to create the annotation file later\n",
    "    for i in range(len(sent)):\n",
    "        \n",
    "        token = sent[i]\n",
    "        curr_word = token[0]\n",
    "        curr_lemma = token[1]\n",
    "        curr_POS = token[2]\n",
    "        \n",
    "        if curr_word != \"#\": # if the given token is not the end of a sentence\n",
    "        \n",
    "            window = sent[i + 1:i + 3]\n",
    "\n",
    "            \n",
    "            if check_macro('NOM', token): # if the given token is a suffixless nominal\n",
    "                \n",
    "                if check_macro('Noun_tree', token): # if the given token is a noun, or a plural adjective, participle\n",
    "                    curr_POS = NOUN_rules(window, curr_POS, curr_word, token)\n",
    "                    # print(token)\n",
    "\n",
    "                elif check_macro('Adj_tree', token): # if the given token is a singular adjective or participle\n",
    "                    curr_POS = ADJ_rules(window, curr_POS)\n",
    "        \n",
    "                elif check_macro('Num_tree', token): # if the given token is a numeral\n",
    "                    curr_POS = NUM_rules(window, curr_POS)\n",
    "                \n",
    "                new_token = curr_word + ' ' + curr_lemma + ' ' + curr_POS\n",
    "                to_write_later.append( (window, new_token ) ) # a tuple of the window of the given token and the full (novel) annotation of the token\n",
    "                \n",
    "        new_sent.append(curr_word + '/' + curr_lemma + '/' + curr_POS)\n",
    "    \n",
    "    return (new_sent, to_write_later)\n",
    "\n",
    "\n",
    "    # print(' '.join(new_sent))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_annot_file(new_sent, window, outp, i):\n",
    "    outp.writelines(str(i) + '. ' + ' '.join([token.split('/')[0] for token in new_sent])+ '\\n')\n",
    "    [outp.writelines(('-' + nom.split()[0] + ' ' + window[0][0] + ' ' + window[1][0] + '\\n', nom + '\\n', nom + '\\n', nom + '\\n')) for (window, nom) in to_write]\n",
    "    outp.writelines(('\\n', '\\n'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 sent, demo\n",
    "new_sent, to_write = nom_or_what('a/a/DET 20./20./MN.NOM század/század/FN.NOM kiemelkedő/kiemelkedik/IK.IGE._OKEP.NOM irodalmi/irodalmi/MN.NOM egyéniségeit/egyéniség/FN.PSe3i.ACC is/is/HA ././SPUNCT')\n",
    "with open ('egymondat', 'w') as outp:\n",
    "    write_to_annot_file(new_sent,to_write, outp, 1)\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 sents, demo, file\n",
    "with open(input_file, 'r') as inp, open(output_file, 'w') as outp:\n",
    "    for i, line in enumerate(inp, start = 1):\n",
    "        new_sent, to_write = nom_or_what(line)\n",
    "        write_to_annot_file(new_sent, to_write, outp, i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_annot_with_outp():\n",
    "    annot = open(orig_annot_file, 'r').readlines()\n",
    "    outp = open(output_file, 'r').readlines()\n",
    "    merged_outp = open(merged_output_file, 'w')\n",
    "    print(len(annot))\n",
    "    print(len(outp))\n",
    "    j = -1\n",
    "    for i in range(len(annot)):\n",
    "        if i == j:\n",
    "            merged_outp.write(outp[i])\n",
    "        else:\n",
    "            merged_outp.write(annot[i])\n",
    "        if annot[i].startswith('-'):\n",
    "            j = i+3\n",
    "    merged_outp.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge_annot_with_outp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13286\n",
      "13288\n"
     ]
    }
   ],
   "source": [
    "merge_annot_with_outp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nom_or_what(annotated_file):\n",
    "    manu_manu = {\"nom_nom\":0,\"nom_suff\":0,\"suff_nom\":0,\"gen_gen\":0,\"gen_suff\":0,\n",
    "                 \"suff_suff\":0,\"suff_gen\":0,\"none_none\":0,\"defnone_defnone\":0,\"defnone_none\":0,\"none_defnone\":0, \"other\":0}\n",
    "    manu_algo = {\"nom_nom\":0,\"nom_suff\":0,\"suff_nom\":0,\"gen_gen\":0,\"gen_suff\":0,\n",
    "                 \"suff_suff\":0,\"suff_gen\":0,\"none_none\":0,\"defnone_defnone\":0,\"defnone_none\":0,\"none_defnone\":0, \"other\":0}\n",
    "    full_algo = {\"nom_nom\":0,\"nom_suff\":0,\"suff_nom\":0,\"gen_gen\":0,\"gen_suff\":0,\n",
    "                 \"suff_suff\":0,\"suff_gen\":0,\"none_none\":0,\"defnone_defnone\":0,\"defnone_none\":0,\"none_defnone\":0, \"other\":0}\n",
    "    nom_nom = 0\n",
    "    nom_suff = 0 \n",
    "    suff_nom = 0\n",
    "    gen_gen = 0\n",
    "    gen_suff = 0 \n",
    "    suff_suff = 0 \n",
    "    suff_gen = 0\n",
    "    none_none = 0\n",
    "    defnone_defnone = 0\n",
    "    defnone_none = 0\n",
    "    none_defnone = 0\n",
    "    xne = 0\n",
    "    other = 0\n",
    "    postag =0\n",
    "    vok = 0    \n",
    "    full_nom_nom = 0\n",
    "    full_nom_suff = 0 \n",
    "    full_suff_nom = 0\n",
    "    full_gen_gen = 0\n",
    "    full_gen_suff = 0 \n",
    "    full_suff_suff = 0 \n",
    "    full_suff_gen = 0\n",
    "    full_none_none = 0\n",
    "    full_defnone_defnone = 0\n",
    "    full_defnone_none = 0\n",
    "    full_none_defnone = 0\n",
    "    full_other = 0\n",
    "    full_nom_nom_algo = 0\n",
    "    full_nom_suff_algo = 0 \n",
    "    full_suff_nom_algo = 0\n",
    "    full_gen_gen_algo = 0\n",
    "    full_gen_suff_algo = 0 \n",
    "    full_suff_suff_algo = 0 \n",
    "    full_suff_gen_algo = 0\n",
    "    full_none_none_algo = 0\n",
    "    full_defnone_defnone_algo = 0\n",
    "    full_defnone_none_algo = 0\n",
    "    full_none_defnone_algo = 0\n",
    "    full_other_algo = 0\n",
    "    postag_full_algo = 0\n",
    "    xne_full_algo = 0\n",
    "    vok_full_algo = 0\n",
    "    inf = open(annotated_file, 'r')\n",
    "    for line in inf:\n",
    "        if len(line)>2:\n",
    "            if re.search(\"^[^0-9\\-]\", line.strip()):\n",
    "                annot_manual_full_sent_line = line\n",
    "                annot_manual_full_sent = line.split()[2]\n",
    "                annot_manual_window_line = inf.readline()\n",
    "                annot_manual_window = annot_manual_window_line.strip().split()[2]\n",
    "                annot_automatic_line = inf.readline()\n",
    "                annot_automatic = annot_automatic_line.strip().split()[2]\n",
    "                if annot_manual_full_sent.endswith('postag_hiba'):\n",
    "                    postag+=1\n",
    "                elif annot_manual_full_sent.endswith(\"xne\"):\n",
    "                    xne+=1\n",
    "                elif annot_manual_full_sent.endswith('vok'):\n",
    "                    vok+= 1\n",
    "                else:\n",
    "                    # egyezés, teljes mondat - ablak, kézi:\n",
    "                    if annot_manual_full_sent == annot_manual_window:\n",
    "                        if annot_manual_full_sent.endswith(\"nom\"):\n",
    "                            manu_manu[\"nom_nom\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"gen\"):\n",
    "                            manu_manu[\"gen_gen\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"suff\"):\n",
    "                            manu_manu[\"suff_suff\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"defnone\"):\n",
    "                            manu_manu[\"defnone_defnone\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\".none\"):\n",
    "                            manu_manu[\"none_none\"]+=1\n",
    "                            \n",
    "                    else:\n",
    "                        # ha nem egyezik, nem anmk, nem vok, nem postag\n",
    "                        if annot_manual_full_sent.endswith(\"nom\") and annot_manual_window.endswith(\"suff\"):\n",
    "                            manu_manu[\"nom_suff\"]+= 1\n",
    "                        elif annot_manual_full_sent.endswith(\"gen\") and annot_manual_window.endswith(\"suff\"):\n",
    "                            manu_manu[\"gen_suff\"]+= 1\n",
    "                        elif annot_manual_full_sent.endswith(\"suff\") and annot_manual_window.endswith(\"nom\"):\n",
    "                            manu_manu[\"suff_nom\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"suff\") and annot_manual_window.endswith(\"gen\"):\n",
    "                            manu_manu[\"suff_gen\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"defnone\") and annot_manual_window.endswith(\".none\"):\n",
    "                            manu_manu[\"defnone_none\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\".none\") and annot_manual_window.endswith(\"defnone\"):\n",
    "                            manu_manu[\"none_defnone\"]+=1\n",
    "                        else:\n",
    "                            manu_manu[\"other\"]+=1\n",
    "                            \n",
    "                    # egyezés, teljes mondat - algo:\n",
    "                    if annot_manual_full_sent == annot_automatic:\n",
    "                        if annot_manual_full_sent.endswith(\"nom\"):\n",
    "                            full_algo[\"nom_nom\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"gen\"):\n",
    "                            full_algo[\"gen_gen\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"suff\"):\n",
    "                            full_algo[\"suff_suff\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"defnone\"):\n",
    "                            full_algo[\"defnone_defnone\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\".none\"):\n",
    "                            full_algo[\"none_none\"]+=1\n",
    "                    else:\n",
    "                        # ha nem egyezik, nem anmk, nem vok, nem postag\n",
    "                        if annot_manual_full_sent.endswith(\"nom\") and annot_automatic.endswith(\"suff\"):\n",
    "                            full_algo[\"nom_suff\"]+= 1\n",
    "                        elif annot_manual_full_sent.endswith(\"gen\") and annot_automatic.endswith(\"suff\"):\n",
    "                            full_algo[\"gen_suff\"]+= 1\n",
    "                        elif annot_manual_full_sent.endswith(\"suff\") and annot_automatic.endswith(\"nom\"):\n",
    "                            full_algo[\"suff_nom\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"suff\") and annot_automatic.endswith(\"gen\"):\n",
    "                            full_algo[\"suff_gen\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\"defnone\") and annot_automatic.endswith(\".none\"):\n",
    "                            full_algo[\"defnone_none\"]+=1\n",
    "                        elif annot_manual_full_sent.endswith(\".none\") and annot_automatic.endswith(\"defnone\"):\n",
    "                            full_algo[\"none_defnone\"]+=1\n",
    "                        else:\n",
    "                            full_algo[\"other\"]+=1\n",
    "                            \n",
    "                    # egyezés, kézi ablak - algo:\n",
    "                    if annot_manual_window == annot_automatic:\n",
    "                        if annot_manual_window.endswith(\"nom\"):\n",
    "                            manu_algo[\"nom_nom\"]+=1\n",
    "                        elif annot_manual_window.endswith(\"gen\"):\n",
    "                            manu_algo[\"gen_gen\"]+=1\n",
    "                        elif annot_manual_window.endswith(\"suff\"):\n",
    "                            manu_algo[\"suff_suff\"]+=1\n",
    "                        elif annot_manual_window.endswith(\"defnone\"):\n",
    "                            manu_algo[\"defnone_defnone\"]+=1\n",
    "                        elif annot_manual_window.endswith(\".none\"):\n",
    "                            manu_algo[\"none_none\"]+=1\n",
    "                    else:\n",
    "                        # ha nem egyezik, nem anmk, nem vok, nem postag\n",
    "                        if annot_manual_window.endswith(\"nom\") and annot_automatic.endswith(\"suff\"):\n",
    "                            manu_algo[\"nom_suff\"]+= 1\n",
    "                        elif annot_manual_window.endswith(\"gen\") and annot_automatic.endswith(\"suff\"):\n",
    "                            manu_algo[\"gen_suff\"]+= 1\n",
    "                        elif annot_manual_window.endswith(\"suff\") and annot_automatic.endswith(\"nom\"):\n",
    "                            manu_algo[\"suff_nom\"]+=1\n",
    "                        elif annot_manual_window.endswith(\"suff\") and annot_automatic.endswith(\"gen\"):\n",
    "                            manu_algo[\"suff_gen\"]+=1\n",
    "                        elif annot_manual_window.endswith(\"defnone\") and annot_automatic.endswith(\".none\"):\n",
    "                            manu_algo[\"defnone_none\"]+=1\n",
    "                        elif annot_manual_window.endswith(\".none\") and annot_automatic.endswith(\"defnone\"):\n",
    "                            manu_algo[\"none_defnone\"]+=1\n",
    "                        else:\n",
    "                            manu_algo[\"other\"]+=1\n",
    "                            \n",
    "    print(\"xne = \" + str(xne))\n",
    "    print(\"postag = \" + str(postag))\n",
    "    print(\"vok = \"+ str(vok))\n",
    "    print()\n",
    "    print(\"Kézi-kézi eredmények:\")\n",
    "    print()\n",
    "    for key in manu_manu:\n",
    "        print(key,manu_manu[key])\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Kézi - algoritmus:\")\n",
    "    print()\n",
    "    for key in manu_algo:\n",
    "        print(key,manu_algo[key])\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Teljes mondat - algoritmus:\")\n",
    "    print()\n",
    "    for key in full_algo:\n",
    "        print(key,full_algo[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2551\n",
      "amnk = 42\n",
      "postag = 128\n",
      "vok = 34\n",
      "\n",
      "Kézi-kézi eredmények:\n",
      "\n",
      "nom_nom 626\n",
      "nom_nulla 187\n",
      "nulla_nom 0\n",
      "gen_gen 266\n",
      "gen_nulla 54\n",
      "nulla_nulla 17\n",
      "nulla_gen 0\n",
      "semmi_semmi 762\n",
      "defsemmi_defsemmi 6\n",
      "defsemmi_semmi 0\n",
      "semmi_defsemmi 397\n",
      "other 31\n",
      "\n",
      "Counter a kézi-kéziben:\n",
      "2346\n",
      "\n",
      "Kézi - algoritmus:\n",
      "\n",
      "nom_nom 621\n",
      "nom_nulla 13\n",
      "nulla_nom 19\n",
      "gen_gen 257\n",
      "gen_nulla 4\n",
      "nulla_nulla 247\n",
      "nulla_gen 4\n",
      "semmi_semmi 623\n",
      "defsemmi_defsemmi 364\n",
      "defsemmi_semmi 42\n",
      "semmi_defsemmi 131\n",
      "other 21\n",
      "\n",
      "Counter a kézi-algóban:\n",
      "2346\n",
      "\n",
      "Teljes mondat - algoritmus:\n",
      "\n",
      "nom_nom 630\n",
      "nom_nulla 181\n",
      "nulla_nom 0\n",
      "gen_gen 258\n",
      "gen_nulla 54\n",
      "nulla_nulla 16\n",
      "nulla_gen 1\n",
      "semmi_semmi 663\n",
      "defsemmi_defsemmi 6\n",
      "defsemmi_semmi 0\n",
      "semmi_defsemmi 482\n",
      "other 55\n",
      "Counter a teljes mondat - algóban:\n",
      "2346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_nom_or_what(\"test_1000_merged_output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
